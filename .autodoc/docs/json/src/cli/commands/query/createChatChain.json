{
  "fileName": "createChatChain.ts",
  "filePath": "src/cli/commands/query/createChatChain.ts",
  "url": "https://github.com/context-labs/autodoc/src/cli/commands/query/createChatChain.ts",
  "summary": "This code defines a function `makeChain` that creates a chatbot for answering questions about a software project. The chatbot is built using the `ChatVectorDBQAChain` class, which combines two separate language models: a question generator and a document chain.\n\nThe question generator is an instance of the `LLMChain` class, which uses the OpenAIChat API to generate standalone questions based on a given conversation history. The `CONDENSE_PROMPT` template is used to format the input for the language model.\n\nThe document chain is created using the `loadQAChain` function, which takes an instance of the OpenAIChat API and a prompt template as input. The `makeQAPrompt` function generates this template, which instructs the language model to provide a conversational answer with hyperlinks to the project's GitHub repository. The answer should be tailored to the target audience and include code examples when appropriate.\n\nThe `makeChain` function takes the following parameters:\n\n- `projectName`: The name of the software project.\n- `repositoryUrl`: The URL of the project's GitHub repository.\n- `contentType`: The type of content the chatbot is trained on (e.g., code, documentation).\n- `chatPrompt`: Additional instructions for answering questions about the content.\n- `targetAudience`: The intended audience for the chatbot's answers (e.g., developers, users).\n- `vectorstore`: An instance of the `HNSWLib` class for storing and searching vectors.\n- `llms`: An array of language models (e.g., GPT-3, GPT-4).\n- `onTokenStream`: An optional callback function to handle streaming tokens.\n\nExample usage:\n\n```javascript\nconst chatbot = makeChain(\n  \"autodoc\",\n  \"https://github.com/autodoc/autodoc\",\n  \"code\",\n  \"\",\n  \"developer\",\n  vectorstore,\n  [gpt3, gpt4],\n  (token) => console.log(token)\n);\n```\n\nThis creates a chatbot that can answer questions about the \"autodoc\" project, using the provided language models and vector store.",
  "questions": "1. **Question:** What is the purpose of the `makeChain` function and what are its input parameters?\n   **Answer:** The `makeChain` function is used to create a new `ChatVectorDBQAChain` instance, which is responsible for generating questions and answers based on the given input parameters. The input parameters include `projectName`, `repositoryUrl`, `contentType`, `chatPrompt`, `targetAudience`, `vectorstore`, `llms`, and an optional `onTokenStream` callback function.\n\n2. **Question:** What are the roles of `CONDENSE_PROMPT` and `QA_PROMPT` in the code?\n   **Answer:** `CONDENSE_PROMPT` is a template for generating a standalone question from a given chat history and follow-up input. `QA_PROMPT` is a template for generating a conversational answer with hyperlinks back to GitHub, based on the given context and question. Both templates are used in the `LLMChain` and `loadQAChain` instances, respectively.\n\n3. **Question:** How does the `onTokenStream` callback function work and when is it used?\n   **Answer:** The `onTokenStream` callback function is an optional parameter in the `makeChain` function. It is used to handle the streaming of tokens generated by the OpenAIChat instance. If provided, it will be called with each new token generated during the chat process, allowing developers to handle or process the tokens in real-time."
}