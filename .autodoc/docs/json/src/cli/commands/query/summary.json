{
  "folderName": "query",
  "folderPath": ".autodoc\\docs\\json\\src\\cli\\commands\\query",
  "url": "https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands\\query",
  "files": [
    {
      "fileName": "createChatChain.ts",
      "filePath": "src\\cli\\commands\\query\\createChatChain.ts",
      "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\query\\createChatChain.ts",
      "summary": "This code defines a function `makeChain` that creates a chatbot for answering questions about a software project called `projectName`. The chatbot is trained on the content of the project, which is located at `repositoryUrl`. The content type of the project is specified by the `contentType` parameter. The chatbot is designed to provide conversational answers with hyperlinks back to GitHub, including code examples and links to the examples where appropriate. The target audience for the chatbot is specified by the `targetAudience` parameter.\n\nThe `makeChain` function takes several parameters:\n\n- `projectName`: The name of the software project.\n- `repositoryUrl`: The URL of the project's repository.\n- `contentType`: The type of content the chatbot is trained on.\n- `chatPrompt`: Additional instructions for answering questions about the content type.\n- `targetAudience`: The intended audience for the chatbot's answers.\n- `vectorstore`: An instance of HNSWLib for efficient nearest neighbor search.\n- `llms`: An array of LLMModels, which are language models used for generating answers.\n- `onTokenStream`: An optional callback function that is called when a new token is generated by the language model.\n\nThe `makeChain` function first creates a question generator using the `LLMChain` class. This generator is responsible for rephrasing follow-up questions to be standalone questions. It uses the `CONDENSE_PROMPT` template, which is defined at the beginning of the code.\n\nNext, the function creates a `QA_PROMPT` template using the `makeQAPrompt` function. This template is used to generate answers to the questions in a conversational manner, with hyperlinks back to GitHub and code examples where appropriate.\n\nFinally, the function creates and returns a new instance of the `ChatVectorDBQAChain` class, which combines the question generator and the document chain to create a chatbot that can answer questions about the software project. The chatbot uses the `vectorstore` for efficient nearest neighbor search and the `llms` language models for generating answers. If the `onTokenStream` callback is provided, it will be called when a new token is generated by the language model.",
      "questions": "1. **Question:** What is the purpose of the `makeChain` function and what are its input parameters?\n\n   **Answer:** The `makeChain` function is used to create a `ChatVectorDBQAChain` instance, which is responsible for generating questions and answers based on the given input parameters. The input parameters include `projectName`, `repositoryUrl`, `contentType`, `chatPrompt`, `targetAudience`, `vectorstore`, `llms`, and an optional `onTokenStream` function.\n\n2. **Question:** What are the roles of `CONDENSE_PROMPT` and `QA_PROMPT` in this code?\n\n   **Answer:** `CONDENSE_PROMPT` is a template for generating standalone questions from a given chat history and follow-up question. `QA_PROMPT` is a template for generating conversational answers with hyperlinks to GitHub, based on the provided context and question. Both templates are used in the `LLMChain` and `loadQAChain` instances, respectively.\n\n3. **Question:** How does the `onTokenStream` function work and when is it used?\n\n   **Answer:** The `onTokenStream` function is an optional callback that can be provided to the `makeChain` function. It is used to handle the streaming of tokens generated by the OpenAIChat instance. If provided, it will be called with each new token generated during the chat process.",
      "checksum": "6869048a06de62499933b14c37cddc1d"
    },
    {
      "fileName": "index.ts",
      "filePath": "src\\cli\\commands\\query\\index.ts",
      "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\query\\index.ts",
      "summary": "This code defines a chatbot interface for the Autodoc project, which allows users to ask questions related to a specific codebase and receive answers in a conversational manner. The chatbot uses a combination of the `inquirer` library for user input, `marked` and `marked-terminal` for rendering Markdown output, and the `langchain` library for handling natural language processing tasks.\n\nThe `query` function is the main entry point for the chatbot. It takes two arguments: an `AutodocRepoConfig` object containing information about the code repository, and an `AutodocUserConfig` object containing user-specific settings. The function initializes a vector store using the `HNSWLib` and `OpenAIEmbeddings` classes, and creates a chat chain using the `makeChain` function.\n\nThe chatbot interface is displayed using the `displayWelcomeMessage` function, which prints a welcome message to the console. The `getQuestion` function is used to prompt the user for a question using the `inquirer` library. The chatbot then enters a loop, where it processes the user's question, generates a response using the chat chain, and displays the response as Markdown in the terminal.\n\nIf an error occurs during the processing of a question, the chatbot will display an error message and continue to prompt the user for a new question. The loop continues until the user types 'exit', at which point the chatbot terminates.\n\nHere's an example of how the `query` function might be used:\n\n```javascript\nimport { query } from './autodoc';\n\nconst repoConfig = {\n  name: 'MyProject',\n  repositoryUrl: 'https://github.com/user/myproject',\n  output: 'path/to/output',\n  contentType: 'code',\n  chatPrompt: 'Ask me anything about MyProject',\n  targetAudience: 'developers',\n};\n\nconst userConfig = {\n  llms: 'path/to/llms',\n};\n\nquery(repoConfig, userConfig);\n```\n\nThis example would initialize the chatbot with the specified repository and user configurations, and start the chatbot interface for the user to ask questions about the \"MyProject\" codebase.",
      "questions": "1. **What is the purpose of the `query` function in this code?**\n\n   The `query` function is responsible for handling user interactions with the chatbot. It takes in an AutodocRepoConfig object and an AutodocUserConfig object, sets up the necessary data structures, and then enters a loop where it prompts the user for questions, processes them, and displays the results.\n\n2. **How does the code handle rendering Markdown text in the terminal?**\n\n   The code uses the `marked` library along with a custom `TerminalRenderer` to render Markdown text in the terminal. The `marked` library is configured with the custom renderer using `marked.setOptions({ renderer: new TerminalRenderer() });`.\n\n3. **What is the purpose of the `chatHistory` variable and how is it used?**\n\n   The `chatHistory` variable is an array that stores the history of questions and answers in the chat session. It is used to keep track of the conversation between the user and the chatbot. When a new question is asked, the chat history is passed to the `chain.call()` function, and the new question and its corresponding answer are added to the `chatHistory` array.",
      "checksum": "19807a33957666422f31136970c37245"
    }
  ],
  "folders": [],
  "summary": "The `query` folder in the Autodoc project contains code for creating a chatbot that can answer questions about a specific software project in a conversational manner. The chatbot is trained on the content of the project and provides answers with hyperlinks back to GitHub, including code examples and links to the examples where appropriate.\n\nThe main entry point for the chatbot is the `query` function in `index.ts`. It takes two arguments: an `AutodocRepoConfig` object containing information about the code repository, and an `AutodocUserConfig` object containing user-specific settings. The function initializes a vector store and creates a chat chain using the `makeChain` function from `createChatChain.ts`.\n\nHere's an example of how the `query` function might be used:\n\n```javascript\nimport { query } from './autodoc';\n\nconst repoConfig = {\n  name: 'MyProject',\n  repositoryUrl: 'https://github.com/user/myproject',\n  output: 'path/to/output',\n  contentType: 'code',\n  chatPrompt: 'Ask me anything about MyProject',\n  targetAudience: 'developers',\n};\n\nconst userConfig = {\n  llms: 'path/to/llms',\n};\n\nquery(repoConfig, userConfig);\n```\n\nThis example initializes the chatbot with the specified repository and user configurations and starts the chatbot interface for the user to ask questions about the \"MyProject\" codebase.\n\nThe `createChatChain.ts` file defines the `makeChain` function, which creates a chatbot for answering questions about a software project. The chatbot is designed to provide conversational answers with hyperlinks back to GitHub, including code examples and links to the examples where appropriate. The target audience for the chatbot is specified by the `targetAudience` parameter.\n\nThe `makeChain` function takes several parameters, such as `projectName`, `repositoryUrl`, `contentType`, `chatPrompt`, `targetAudience`, `vectorstore`, `llms`, and `onTokenStream`. It first creates a question generator using the `LLMChain` class, then creates a `QA_PROMPT` template using the `makeQAPrompt` function, and finally creates and returns a new instance of the `ChatVectorDBQAChain` class, which combines the question generator and the document chain to create a chatbot that can answer questions about the software project.\n\nIn summary, the code in the `query` folder is responsible for creating a chatbot that can answer questions about a specific software project in a conversational manner. The chatbot uses a combination of natural language processing techniques and efficient nearest neighbor search to generate accurate and relevant answers for the user.",
  "questions": "",
  "checksum": "9e0d0f111bf588e2df66862dce9db288"
}