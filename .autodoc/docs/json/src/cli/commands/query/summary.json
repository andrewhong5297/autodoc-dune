{
  "folderName": "query",
  "folderPath": ".autodoc/docs/json/src/cli/commands/query",
  "url": "https://github.com/context-labs/autodoc/.autodoc/docs/json/src/cli/commands/query",
  "files": [
    {
      "fileName": "createChatChain.ts",
      "filePath": "src/cli/commands/query/createChatChain.ts",
      "url": "https://github.com/context-labs/autodoc/src/cli/commands/query/createChatChain.ts",
      "summary": "This code defines a function `makeChain` that creates a chatbot for answering questions about a software project. The chatbot is built using the `ChatVectorDBQAChain` class, which combines two separate language models: a question generator and a document chain.\n\nThe question generator is an instance of the `LLMChain` class, which uses the OpenAIChat API to generate standalone questions based on a given conversation history. The `CONDENSE_PROMPT` template is used to format the input for the language model.\n\nThe document chain is created using the `loadQAChain` function, which takes an instance of the OpenAIChat API and a prompt template as input. The `makeQAPrompt` function generates this template, which instructs the language model to provide a conversational answer with hyperlinks to the project's GitHub repository. The answer should be tailored to the target audience and include code examples when appropriate.\n\nThe `makeChain` function takes the following parameters:\n\n- `projectName`: The name of the software project.\n- `repositoryUrl`: The URL of the project's GitHub repository.\n- `contentType`: The type of content the chatbot is trained on (e.g., code, documentation).\n- `chatPrompt`: Additional instructions for answering questions about the content.\n- `targetAudience`: The intended audience for the chatbot's answers (e.g., developers, users).\n- `vectorstore`: An instance of the `HNSWLib` class for storing and searching vectors.\n- `llms`: An array of language models (e.g., GPT-3, GPT-4).\n- `onTokenStream`: An optional callback function to handle streaming tokens.\n\nExample usage:\n\n```javascript\nconst chatbot = makeChain(\n  \"autodoc\",\n  \"https://github.com/autodoc/autodoc\",\n  \"code\",\n  \"\",\n  \"developer\",\n  vectorstore,\n  [gpt3, gpt4],\n  (token) => console.log(token)\n);\n```\n\nThis creates a chatbot that can answer questions about the \"autodoc\" project, using the provided language models and vector store.",
      "questions": "1. **Question:** What is the purpose of the `makeChain` function and what are its input parameters?\n   **Answer:** The `makeChain` function is used to create a new `ChatVectorDBQAChain` instance, which is responsible for generating questions and answers based on the given input parameters. The input parameters include `projectName`, `repositoryUrl`, `contentType`, `chatPrompt`, `targetAudience`, `vectorstore`, `llms`, and an optional `onTokenStream` callback function.\n\n2. **Question:** What are the roles of `CONDENSE_PROMPT` and `QA_PROMPT` in the code?\n   **Answer:** `CONDENSE_PROMPT` is a template for generating a standalone question from a given chat history and follow-up input. `QA_PROMPT` is a template for generating a conversational answer with hyperlinks back to GitHub, based on the given context and question. Both templates are used in the `LLMChain` and `loadQAChain` instances, respectively.\n\n3. **Question:** How does the `onTokenStream` callback function work and when is it used?\n   **Answer:** The `onTokenStream` callback function is an optional parameter in the `makeChain` function. It is used to handle the streaming of tokens generated by the OpenAIChat instance. If provided, it will be called with each new token generated during the chat process, allowing developers to handle or process the tokens in real-time."
    },
    {
      "fileName": "index.ts",
      "filePath": "src/cli/commands/query/index.ts",
      "url": "https://github.com/context-labs/autodoc/src/cli/commands/query/index.ts",
      "summary": "This code defines a chatbot interface for the Autodoc project, which allows users to ask questions related to a specific codebase and receive answers in a conversational manner. The chatbot uses a language model to generate responses based on the user's input and the codebase documentation.\n\nThe code starts by importing necessary libraries and setting up the `marked` library with a custom terminal renderer for displaying Markdown content. It then defines a `chatHistory` array to store the conversation history between the user and the chatbot.\n\nThe `displayWelcomeMessage` function is used to display a welcome message to the user when they start the chatbot. The `clearScreenAndMoveCursorToTop` function clears the terminal screen and moves the cursor to the top.\n\nThe main function, `query`, takes two arguments: `AutodocRepoConfig` and `AutodocUserConfig`. It initializes the `vectorStore` by loading pre-trained embeddings and creates a `chain` object using the `makeChain` function. This chain object is responsible for generating responses based on the user's input.\n\nThe `getQuestion` function uses the `inquirer` library to prompt the user for a question. The main loop of the chatbot starts by getting the user's question and continues until the user types 'exit'. Inside the loop, the code updates the spinner text to 'Thinking...' and calls the `chain` object with the user's question and chat history. The response is then displayed in Markdown format using the `marked` library.\n\nIf an error occurs during the process, the chatbot displays an error message and prompts the user for another question.\n\nExample usage:\n\n```javascript\nquery(repoConfig, userConfig);\n```\n\nThis chatbot interface can be used in the larger Autodoc project to help users navigate and understand the codebase more efficiently by providing a conversational interface for asking questions and receiving answers.",
      "questions": "1. **What is the purpose of the `query` function and what are its input parameters?**\n\n   The `query` function is used to interact with the chatbot, taking user input and providing responses based on the given codebase. It takes two input parameters: an `AutodocRepoConfig` object containing information about the repository, and an `AutodocUserConfig` object containing user-specific configuration.\n\n2. **How does the `vectorStore` work and what is its role in the code?**\n\n   The `vectorStore` is an instance of HNSWLib loaded with data from the specified output directory and using OpenAIEmbeddings. It is used to store and retrieve vector representations of the codebase, which are then used by the `makeChain` function to generate responses to user questions.\n\n3. **How does the chat history work and what is its purpose?**\n\n   The `chatHistory` is an array of string pairs, where each pair represents a user question and the corresponding chatbot response. It is used to store the conversation history between the user and the chatbot, allowing the chatbot to provide context-aware responses based on previous interactions."
    }
  ],
  "folders": [],
  "summary": "The `query` folder in the Autodoc project contains code for creating a chatbot interface that allows users to ask questions related to a specific codebase and receive answers in a conversational manner. The chatbot uses a language model to generate responses based on the user's input and the codebase documentation.\n\nIn `createChatChain.ts`, the `makeChain` function is defined, which creates a chatbot using the `ChatVectorDBQAChain` class. This class combines two separate language models: a question generator and a document chain. The question generator is an instance of the `LLMChain` class, which uses the OpenAIChat API to generate standalone questions based on a given conversation history. The document chain is created using the `loadQAChain` function, which takes an instance of the OpenAIChat API and a prompt template as input.\n\nExample usage of `makeChain`:\n\n```javascript\nconst chatbot = makeChain(\n  \"autodoc\",\n  \"https://github.com/autodoc/autodoc\",\n  \"code\",\n  \"\",\n  \"developer\",\n  vectorstore,\n  [gpt3, gpt4],\n  (token) => console.log(token)\n);\n```\n\nIn `index.ts`, the main chatbot interface is defined. It starts by importing necessary libraries and setting up the `marked` library with a custom terminal renderer for displaying Markdown content. The main function, `query`, takes two arguments: `AutodocRepoConfig` and `AutodocUserConfig`. It initializes the `vectorStore` by loading pre-trained embeddings and creates a `chain` object using the `makeChain` function. This chain object is responsible for generating responses based on the user's input.\n\nThe main loop of the chatbot starts by getting the user's question and continues until the user types 'exit'. Inside the loop, the code updates the spinner text to 'Thinking...' and calls the `chain` object with the user's question and chat history. The response is then displayed in Markdown format using the `marked` library.\n\nExample usage of the chatbot interface:\n\n```javascript\nquery(repoConfig, userConfig);\n```\n\nThis chatbot interface can be used in the larger Autodoc project to help users navigate and understand the codebase more efficiently by providing a conversational interface for asking questions and receiving answers.",
  "questions": ""
}