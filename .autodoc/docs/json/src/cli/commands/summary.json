{
  "folderName": "commands",
  "folderPath": ".autodoc\\docs\\json\\src\\cli\\commands",
  "url": "https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands",
  "files": [],
  "folders": [
    {
      "folderName": "estimate",
      "folderPath": ".autodoc\\docs\\json\\src\\cli\\commands\\estimate",
      "url": "https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands\\estimate",
      "files": [
        {
          "fileName": "index.ts",
          "filePath": "src\\cli\\commands\\estimate\\index.ts",
          "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\estimate\\index.ts",
          "summary": "The `estimate` function in this code is responsible for providing an estimated cost of processing a given repository using the Autodoc project. It takes an `AutodocRepoConfig` object as input, which contains various configuration options such as the repository name, URL, root directory, output directory, and other settings related to the processing of the repository.\n\nThe function starts by constructing the path to the JSON output directory, which will be used to store the intermediate results of the processing. It then updates the spinner text to indicate that the cost estimation is in progress.\n\nNext, the `processRepository` function is called with the provided configuration options and a `true` flag to indicate that this is a dry run. This means that the repository will not actually be processed, but the function will return the details of what would happen if it were processed. This is used to calculate the estimated cost of processing the repository.\n\nOnce the dry run is complete, the spinner is updated to show success, and the results are printed using the `printModelDetails` function. The total estimated cost is then calculated using the `totalIndexCostEstimate` function, which takes the values of the `runDetails` object as input.\n\nFinally, the estimated cost is displayed in the console using the `chalk.redBright` function to format the text in a red color. The message also includes a disclaimer that the actual cost may vary and recommends setting a limit in the user's OpenAI account to prevent unexpected charges.\n\nHere's an example of how the `estimate` function might be used in the larger project:\n\n```javascript\nimport { estimate } from './path/to/this/file';\n\nconst config = {\n  name: 'my-repo',\n  repositoryUrl: 'https://github.com/user/my-repo.git',\n  root: './',\n  output: './output',\n  llms: ['en'],\n  ignore: ['.git', 'node_modules'],\n  filePrompt: true,\n  folderPrompt: true,\n  chatPrompt: true,\n  contentType: 'code',\n  targetAudience: 'developers',\n  linkHosted: true,\n};\n\nestimate(config);\n```\n\nThis example would estimate the cost of processing the \"my-repo\" repository with the specified configuration options.",
          "questions": "1. **What is the purpose of the `estimate` function?**\n\n   The `estimate` function is used to perform a dry run of the `processRepository` command to get an estimated price for indexing the given repository. It then prints the model details and the total estimated cost.\n\n2. **What are the parameters passed to the `processRepository` function?**\n\n   The `processRepository` function is called with an object containing the following properties: `name`, `repositoryUrl`, `root`, `output`, `llms`, `ignore`, `filePrompt`, `folderPrompt`, `chatPrompt`, `contentType`, `targetAudience`, and `linkHosted`. Additionally, a second argument `true` is passed to indicate that it's a dry run.\n\n3. **How is the total estimated cost calculated and displayed?**\n\n   The total estimated cost is calculated using the `totalIndexCostEstimate` function, which takes an array of values from the `runDetails` object. The cost is then displayed using `console.log` with `chalk.redBright` for formatting, showing the cost with two decimal places and a note that the actual cost may vary.",
          "checksum": "2b0b3903432ae423bbc597d04b052ecb"
        }
      ],
      "folders": [],
      "summary": "The `estimate` function in `index.ts` is a crucial part of the Autodoc project, as it provides an estimated cost of processing a given repository. It takes an `AutodocRepoConfig` object as input, containing various configuration options such as repository name, URL, root directory, output directory, and other settings related to the processing of the repository.\n\nThe function begins by constructing the path to the JSON output directory, which stores intermediate results of the processing. It then updates the spinner text to indicate that cost estimation is in progress. The `processRepository` function is called with the provided configuration options and a `true` flag, signifying a dry run. This dry run returns the details of what would happen if the repository were processed, which is used to calculate the estimated cost.\n\nUpon completion of the dry run, the spinner is updated to show success, and the results are printed using the `printModelDetails` function. The total estimated cost is calculated using the `totalIndexCostEstimate` function, which takes the values of the `runDetails` object as input.\n\nFinally, the estimated cost is displayed in the console using the `chalk.redBright` function to format the text in red. The message also includes a disclaimer that the actual cost may vary and recommends setting a limit in the user's OpenAI account to prevent unexpected charges.\n\nHere's an example of how the `estimate` function might be used in the larger project:\n\n```javascript\nimport { estimate } from './path/to/this/file';\n\nconst config = {\n  name: 'my-repo',\n  repositoryUrl: 'https://github.com/user/my-repo.git',\n  root: './',\n  output: './output',\n  llms: ['en'],\n  ignore: ['.git', 'node_modules'],\n  filePrompt: true,\n  folderPrompt: true,\n  chatPrompt: true,\n  contentType: 'code',\n  targetAudience: 'developers',\n  linkHosted: true,\n};\n\nestimate(config);\n```\n\nThis example would estimate the cost of processing the \"my-repo\" repository with the specified configuration options.",
      "questions": "",
      "checksum": "4b8fd2b2abaec4959873fc3396c414d8"
    },
    {
      "folderName": "index",
      "folderPath": ".autodoc\\docs\\json\\src\\cli\\commands\\index",
      "url": "https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands\\index",
      "files": [
        {
          "fileName": "convertJsonToMarkdown.ts",
          "filePath": "src\\cli\\commands\\index\\convertJsonToMarkdown.ts",
          "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\index\\convertJsonToMarkdown.ts",
          "summary": "The `convertJsonToMarkdown` function in this code is responsible for converting JSON files containing documentation information into Markdown files. This function is part of the larger Autodoc project, which aims to automate the process of generating documentation for code repositories.\n\nThe function takes an `AutodocRepoConfig` object as input, which contains various configuration options such as the project name, input and output directories, and other settings related to the documentation generation process.\n\nThe code first counts the number of files in the project by traversing the file system using the `traverseFileSystem` utility function. This is done to provide a progress update to the user via the `updateSpinnerText` function.\n\nNext, the `processFile` function is defined, which is responsible for reading the content of each JSON file, parsing it, and converting it into a Markdown format. The function checks if the file has a summary, and if so, it generates the Markdown content with a link to the code on GitHub, the summary, and any questions if present. The output Markdown file is then saved in the specified output directory.\n\nFinally, the `traverseFileSystem` function is called again, this time with the `processFile` function as an argument. This allows the code to process each JSON file in the project and convert it into a Markdown file. Once the process is complete, a success message is displayed to the user using the `spinnerSuccess` function.\n\nExample usage:\n\n```javascript\nconvertJsonToMarkdown({\n  name: \"myProject\",\n  root: \"./input\",\n  output: \"./output\",\n  filePrompt: true,\n  folderPrompt: true,\n  contentType: \"code\",\n  targetAudience: \"developers\",\n  linkHosted: \"https://github.com/user/myProject\",\n});\n```\n\nThis will convert all JSON files in the `./input` directory into Markdown files and save them in the `./output` directory.",
          "questions": "1. **Question:** What is the purpose of the `convertJsonToMarkdown` function and what are the expected inputs?\n   **Answer:** The `convertJsonToMarkdown` function is used to convert JSON files to Markdown files for each code file in the project. It takes an `AutodocRepoConfig` object as input, which contains various properties like projectName, root, output, filePrompt, folderPrompt, contentType, targetAudience, and linkHosted.\n\n2. **Question:** How does the `traverseFileSystem` function work and what is its role in this code?\n   **Answer:** The `traverseFileSystem` function is a utility function that recursively traverses the file system, starting from the inputPath, and processes each file using the provided `processFile` function. In this code, it is used twice: first to count the number of files in the project, and then to create Markdown files for each code file in the project.\n\n3. **Question:** How are the output directories and Markdown files created, and what is the structure of the generated Markdown content?\n   **Answer:** The output directories are created using the `fs.mkdir` function with the `recursive: true` option. The Markdown files are created using the `fs.writeFile` function. The structure of the generated Markdown content includes a link to view the code on GitHub, the summary, and optionally, a list of questions if they exist.",
          "checksum": "79c860becf47b9882441682f0213d534"
        },
        {
          "fileName": "createVectorStore.ts",
          "filePath": "src\\cli\\commands\\index\\createVectorStore.ts",
          "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\index\\createVectorStore.ts",
          "summary": "The code in this file is responsible for processing a directory of text files, splitting the text into chunks, and creating a vector store using the HNSWLib library and OpenAIEmbeddings. This vector store can be used for efficient similarity search and retrieval of documents in the larger project.\n\nThe `processFile` function reads a file's content and creates a `Document` object with the content and metadata (source file path). It returns a Promise that resolves to the created Document.\n\nThe `processDirectory` function is a recursive function that processes a directory and its subdirectories. It reads the files in the directory, and for each file, it checks if it's a directory or a regular file. If it's a directory, the function calls itself with the new directory path. If it's a file, it calls the `processFile` function to create a Document object. The function returns an array of Document objects.\n\nThe `RepoLoader` class extends the `BaseDocumentLoader` class and has a constructor that takes a file path as an argument. It has a `load` method that calls the `processDirectory` function with the given file path and returns the array of Document objects.\n\nThe `createVectorStore` function is an async function that takes an `AutodocRepoConfig` object as an argument, which contains the root directory and output file path. It creates a `RepoLoader` instance with the root directory and loads the documents using the `load` method. It then creates a `RecursiveCharacterTextSplitter` instance with a specified chunk size and chunk overlap and splits the documents into chunks. Finally, it creates a vector store using the HNSWLib library and OpenAIEmbeddings with the processed documents and saves the vector store to the output file path.\n\nExample usage:\n\n```javascript\nconst config = {\n  root: './data/documents',\n  output: './data/vector_store',\n};\n\ncreateVectorStore(config).then(() => {\n  console.log('Vector store created successfully');\n});\n```",
          "questions": "1. **Question:** What is the purpose of the `processFile` function and what does it return?\n   **Answer:** The `processFile` function is an asynchronous function that reads the content of a file given its file path, creates a `Document` object with the file contents and metadata (source file path), and returns a Promise that resolves to the created `Document` object.\n\n2. **Question:** How does the `processDirectory` function work and what does it return?\n   **Answer:** The `processDirectory` function is an asynchronous function that takes a directory path as input, reads all the files and subdirectories within it, and processes them recursively. It returns a Promise that resolves to an array of `Document` objects created from the files in the directory and its subdirectories.\n\n3. **Question:** What is the purpose of the `createVectorStore` function and how does it work?\n   **Answer:** The `createVectorStore` function is an asynchronous function that takes an `AutodocRepoConfig` object as input, which contains the root directory path and output file path. The function loads all the documents from the root directory using the `RepoLoader`, splits the text into chunks using the `RecursiveCharacterTextSplitter`, creates a vector store from the documents using the `HNSWLib` and `OpenAIEmbeddings`, and saves the vector store to the specified output file.",
          "checksum": "a3409c4340753a867c72eebef7626fb9"
        },
        {
          "fileName": "index.ts",
          "filePath": "src\\cli\\commands\\index\\index.ts",
          "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\index\\index.ts",
          "summary": "The code in this file is responsible for processing a given repository and generating documentation in JSON, Markdown, and vector formats. It exports a single function `index` that takes an `AutodocRepoConfig` object as its argument, which contains various configuration options for processing the repository.\n\nThe `index` function performs three main tasks:\n\n1. **Process the repository**: It traverses the repository, calls the LLMS (Language Learning Management System) for each file, and creates JSON files with the results. This is done using the `processRepository` function, which takes the same configuration options as the `index` function. The JSON files are stored in the `output/docs/json/` directory.\n\n   ```javascript\n   updateSpinnerText('Processing repository...');\n   await processRepository({ /* configuration options */ });\n   spinnerSuccess();\n   ```\n\n2. **Create Markdown files**: It converts the generated JSON files into Markdown files using the `convertJsonToMarkdown` function. This function also takes the same configuration options as the `index` function. The Markdown files are stored in the `output/docs/markdown/` directory.\n\n   ```javascript\n   updateSpinnerText('Creating markdown files...');\n   await convertJsonToMarkdown({ /* configuration options */ });\n   spinnerSuccess();\n   ```\n\n3. **Create vector files**: It creates vector files from the generated Markdown files using the `createVectorStore` function. This function also takes the same configuration options as the `index` function. The vector files are stored in the `output/docs/data/` directory.\n\n   ```javascript\n   updateSpinnerText('Create vector files...');\n   await createVectorStore({ /* configuration options */ });\n   spinnerSuccess();\n   ```\n\nThroughout the execution of these tasks, the code uses `updateSpinnerText` and `spinnerSuccess` functions to provide visual feedback on the progress of the tasks.\n\nIn the larger project, this code would be used to automatically generate documentation for a given repository based on the provided configuration options. The generated documentation can then be used for various purposes, such as displaying it on a website or analyzing the content for specific insights.",
          "questions": "1. **What does the `index` function do in this code?**\n\n   The `index` function is the main entry point for the autodoc project. It takes an `AutodocRepoConfig` object as input and performs three main tasks: processing the repository and creating JSON files, converting JSON files to markdown files, and creating vector files.\n\n2. **What is the purpose of the `processRepository`, `convertJsonToMarkdown`, and `createVectorStore` functions?**\n\n   The `processRepository` function traverses the repository, calls LLMS for each file, and creates JSON files with the results. The `convertJsonToMarkdown` function creates markdown files from the generated JSON files. The `createVectorStore` function creates vector files from the markdown files.\n\n3. **What are the different types of prompts (`filePrompt`, `folderPrompt`, `chatPrompt`) used for in this code?**\n\n   These prompts are likely used to interact with the user during the processing of the repository. The `filePrompt` might be used to ask the user for input regarding specific files, the `folderPrompt` for input regarding folders, and the `chatPrompt` for general input or feedback during the processing.",
          "checksum": "4060b1affae5a6c385cda308b3cd1750"
        },
        {
          "fileName": "processRepository.ts",
          "filePath": "src\\cli\\commands\\index\\processRepository.ts",
          "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\index\\processRepository.ts",
          "summary": "The `processRepository` function in this code is responsible for generating summaries and questions for code files and folders in a given repository. It takes an `AutodocRepoConfig` object as input, which contains information about the project, repository URL, input and output paths, language models, and other configurations. An optional `dryRun` parameter can be provided to skip actual API calls and file writing.\n\nThe function starts by initializing the encoding and rate limit for API calls. It then defines two main helper functions: `processFile` and `processFolder`. The `processFile` function is responsible for processing individual code files. It reads the file content, calculates a checksum, and checks if reindexing is needed. If reindexing is required, it creates prompts for summaries and questions, selects the appropriate language model based on the input length, and calls the language model API to generate the summaries and questions. The results are then saved to a JSON file in the output directory.\n\nThe `processFolder` function is responsible for processing folders. It reads the folder content, calculates a checksum, and checks if reindexing is needed. If reindexing is required, it reads the summaries and questions of all files and subfolders in the folder, calls the language model API to generate a summary for the folder, and saves the result to a `summary.json` file in the folder.\n\nThe main function then counts the number of files and folders in the project and processes them using the `traverseFileSystem` utility function. It processes all files first, followed by all folders. Finally, it returns the language model usage statistics.\n\nThe `calculateChecksum` function calculates the checksum of a list of file contents, while the `reindexCheck` function checks if reindexing is needed by comparing the new and old checksums of a file or folder.",
          "questions": "1. **Question:** What is the purpose of the `processRepository` function and what are its inputs and outputs?\n   **Answer:** The `processRepository` function processes a given code repository, generating summaries and questions for each file and folder within the repository. It takes an `AutodocRepoConfig` object and an optional `dryRun` boolean as inputs. The function returns a `Promise` that resolves to an object containing the models used during processing.\n\n2. **Question:** How does the `calculateChecksum` function work and what is its purpose?\n   **Answer:** The `calculateChecksum` function takes an array of file contents as input and calculates a checksum for each file using the MD5 hashing algorithm. It then concatenates all the checksums and calculates a final checksum using MD5 again. The purpose of this function is to generate a unique identifier for the contents of the files, which can be used to determine if the files have changed and need to be reprocessed.\n\n3. **Question:** How does the `reindexCheck` function work and when is it used?\n   **Answer:** The `reindexCheck` function checks if a summary.json file exists in the given file or folder path and compares the stored checksum with the new checksum to determine if the file or folder needs to be reindexed. It is used in the `processFile` and `processFolder` functions to decide whether to regenerate summaries and questions for a file or folder based on changes in their contents.",
          "checksum": "5b3ae9ffad1d4b4a22c6f7fd66bbde6f"
        },
        {
          "fileName": "prompts.ts",
          "filePath": "src\\cli\\commands\\index\\prompts.ts",
          "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\index\\prompts.ts",
          "summary": "This code defines three utility functions that generate prompts for documentation experts working on a project. These functions are used to create documentation for code files and folders within a project. The generated prompts are in markdown format and include specific instructions for the documentation expert.\n\n1. `createCodeFileSummary`: This function generates a prompt for creating a summary of a code file. It takes five parameters: `filePath`, `projectName`, `fileContents`, `contentType`, and `filePrompt`. The function returns a markdown formatted string that includes the file's content and a custom prompt for the documentation expert.\n\nExample usage:\n```javascript\nconst prompt = createCodeFileSummary('path/to/file.js', 'MyProject', 'const x = 10;', 'JavaScript', 'Write a detailed technical explanation of this code.');\n```\n\n2. `createCodeQuestions`: This function generates a prompt for creating a list of questions and answers about a code file. It takes five parameters: `filePath`, `projectName`, `fileContents`, `contentType`, and `targetAudience`. The function returns a markdown formatted string that includes the file's content and a custom prompt for the documentation expert to provide questions and answers.\n\nExample usage:\n```javascript\nconst prompt = createCodeQuestions('path/to/file.js', 'MyProject', 'const x = 10;', 'JavaScript', 'beginner');\n```\n\n3. `folderSummaryPrompt`: This function generates a prompt for creating a summary of a folder containing code files and subfolders. It takes six parameters: `folderPath`, `projectName`, `files`, `folders`, `contentType`, and `folderPrompt`. The `files` parameter is an array of `FileSummary` objects, and the `folders` parameter is an array of `FolderSummary` objects. The function returns a markdown formatted string that includes a list of files and folders with their summaries and a custom prompt for the documentation expert.\n\nExample usage:\n```javascript\nconst prompt = folderSummaryPrompt('path/to/folder', 'MyProject', fileSummaries, folderSummaries, 'JavaScript', 'Write a detailed technical explanation of this folder structure.');\n```\n\nThese functions can be used in the larger project to generate documentation tasks for experts, ensuring consistent formatting and instructions across different parts of the project.",
          "questions": "1. **What is the purpose of the `createCodeFileSummary` function?**\n\n   The `createCodeFileSummary` function generates a string template for a code file summary prompt, which includes the file path, project name, file contents, content type, and a file prompt.\n\n2. **How does the `createCodeQuestions` function differ from the `createCodeFileSummary` function?**\n\n   The `createCodeQuestions` function generates a string template for a code documentation prompt that asks for 3 questions and their answers, while the `createCodeFileSummary` function generates a string template for a code file summary prompt.\n\n3. **What is the role of the `folderSummaryPrompt` function?**\n\n   The `folderSummaryPrompt` function generates a string template for a folder summary prompt, which includes the folder path, project name, lists of files and folders with their summaries, content type, and a folder prompt.",
          "checksum": "e44b82bf4912be69149685a997b6bde3"
        }
      ],
      "folders": [],
      "summary": "The code in this folder is responsible for processing a given repository and generating documentation in JSON, Markdown, and vector formats. It consists of several functions and utilities that work together to automate the documentation generation process.\n\nThe main function, `index`, takes an `AutodocRepoConfig` object as input, which contains various configuration options for processing the repository. It performs three main tasks:\n\n1. **Process the repository**: It calls the `processRepository` function to traverse the repository, generate summaries and questions for code files and folders using the LLMS (Language Learning Management System), and create JSON files with the results. These JSON files are stored in the `output/docs/json/` directory.\n\n2. **Create Markdown files**: It uses the `convertJsonToMarkdown` function to convert the generated JSON files into Markdown files. These Markdown files are stored in the `output/docs/markdown/` directory.\n\n3. **Create vector files**: It calls the `createVectorStore` function to create vector files from the generated Markdown files. These vector files are stored in the `output/docs/data/` directory.\n\nThroughout the execution of these tasks, the code provides visual feedback on the progress of the tasks using `updateSpinnerText` and `spinnerSuccess` functions.\n\nHere's an example of how this code might be used:\n\n```javascript\nindex({\n  name: \"myProject\",\n  root: \"./input\",\n  output: \"./output\",\n  filePrompt: true,\n  folderPrompt: true,\n  contentType: \"code\",\n  targetAudience: \"developers\",\n  linkHosted: \"https://github.com/user/myProject\",\n});\n```\n\nThis will process the repository located at `./input`, generate documentation in JSON, Markdown, and vector formats, and save the results in the `./output` directory.\n\nThe `prompts.ts` file contains utility functions that generate prompts for documentation experts. These functions create markdown formatted strings with specific instructions for the documentation expert, ensuring consistent formatting and instructions across different parts of the project.\n\nIn summary, the code in this folder automates the process of generating documentation for a given repository based on the provided configuration options. The generated documentation can be used for various purposes, such as displaying it on a website or analyzing the content for specific insights.",
      "questions": "",
      "checksum": "376f96417f8cbea6a5ab2463268fe4af"
    },
    {
      "folderName": "init",
      "folderPath": ".autodoc\\docs\\json\\src\\cli\\commands\\init",
      "url": "https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands\\init",
      "files": [
        {
          "fileName": "index.ts",
          "filePath": "src\\cli\\commands\\init\\index.ts",
          "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\init\\index.ts",
          "summary": "This code is responsible for initializing the configuration of the Autodoc project. It provides a template for the configuration and prompts the user to input necessary information to set up the project. The main functionality is exposed through the `init` function, which is an asynchronous function that takes an optional `AutodocRepoConfig` object as an argument.\n\nThe `makeConfigTemplate` function creates a default configuration object with pre-defined values for various properties. It takes an optional `config` parameter and returns a new `AutodocRepoConfig` object with the provided values or default values if not provided.\n\nThe `init` function first checks if an `autodoc.config.json` file already exists in the project root. If it does, the user is prompted to confirm whether they want to overwrite the existing configuration. If the user chooses not to overwrite, the process exits.\n\nNext, the user is prompted to enter the name of their repository, the GitHub URL of their repository, and the LLMs they have access to. The LLMs are language models used for generating documentation. The user can choose between GPT-3.5 Turbo, GPT-4 8K (Early Access), and GPT-4 32K (Early Access).\n\nAfter the user provides the necessary information, a new configuration object is created using the `makeConfigTemplate` function with the user's input. The new configuration is then written to the `autodoc.config.json` file in the project root.\n\nFinally, a success message is displayed, instructing the user to run `doc index` to get started with the Autodoc project.\n\nExample usage:\n\n```javascript\nimport { init } from './path/to/this/file';\n\n// Initialize the configuration with default values\nawait init();\n\n// Initialize the configuration with custom values\nawait init({\n  name: 'My Custom Repository',\n  repositoryUrl: 'https://github.com/user/repo',\n});\n```",
          "questions": "1. **What is the purpose of the `makeConfigTemplate` function?**\n\n   The `makeConfigTemplate` function is used to create a default configuration object for the Autodoc project. It takes an optional `config` parameter of type `AutodocRepoConfig` and returns a new configuration object with default values for various properties.\n\n2. **How does the `init` function work and when is it called?**\n\n   The `init` function is an asynchronous function that initializes the Autodoc configuration by creating an `autodoc.config.json` file in the specified location. It takes an optional `config` parameter of type `AutodocRepoConfig` and prompts the user for input to set the configuration values. It is called when the user wants to set up the Autodoc configuration for their project.\n\n3. **What is the purpose of the `inquirer.prompt` calls in the `init` function?**\n\n   The `inquirer.prompt` calls are used to interactively prompt the user for input to set the configuration values for the Autodoc project. The user is asked for the repository name, repository URL, and the LLMs they have access to. The input is then used to create a new configuration object and write it to the `autodoc.config.json` file.",
          "checksum": "b93831ff1f4023ab61c3bea963a8a112"
        }
      ],
      "folders": [],
      "summary": "The `index.ts` file in the `.autodoc\\docs\\json\\src\\cli\\commands\\init` folder is responsible for initializing the configuration of the Autodoc project. It provides a template for the configuration and prompts the user to input necessary information to set up the project. The main functionality is exposed through the `init` function, which is an asynchronous function that takes an optional `AutodocRepoConfig` object as an argument.\n\nThe `makeConfigTemplate` function creates a default configuration object with pre-defined values for various properties. It takes an optional `config` parameter and returns a new `AutodocRepoConfig` object with the provided values or default values if not provided.\n\nThe `init` function first checks if an `autodoc.config.json` file already exists in the project root. If it does, the user is prompted to confirm whether they want to overwrite the existing configuration. If the user chooses not to overwrite, the process exits.\n\nNext, the user is prompted to enter the name of their repository, the GitHub URL of their repository, and the LLMs they have access to. The LLMs are language models used for generating documentation. The user can choose between GPT-3.5 Turbo, GPT-4 8K (Early Access), and GPT-4 32K (Early Access).\n\nAfter the user provides the necessary information, a new configuration object is created using the `makeConfigTemplate` function with the user's input. The new configuration is then written to the `autodoc.config.json` file in the project root.\n\nFinally, a success message is displayed, instructing the user to run `doc index` to get started with the Autodoc project.\n\nExample usage:\n\n```javascript\nimport { init } from './path/to/this/file';\n\n// Initialize the configuration with default values\nawait init();\n\n// Initialize the configuration with custom values\nawait init({\n  name: 'My Custom Repository',\n  repositoryUrl: 'https://github.com/user/repo',\n});\n```\n\nThis code is essential for setting up the Autodoc project, as it creates the necessary configuration file and gathers user input to customize the project. It works in conjunction with other parts of the project, such as the CLI and the documentation generation process, which rely on the configuration file to function correctly.",
      "questions": "",
      "checksum": "4b8fd2b2abaec4959873fc3396c414d8"
    },
    {
      "folderName": "query",
      "folderPath": ".autodoc\\docs\\json\\src\\cli\\commands\\query",
      "url": "https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands\\query",
      "files": [
        {
          "fileName": "createChatChain.ts",
          "filePath": "src\\cli\\commands\\query\\createChatChain.ts",
          "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\query\\createChatChain.ts",
          "summary": "This code defines a function `makeChain` that creates a chatbot for answering questions about a software project called `projectName`. The chatbot is trained on the content of the project, which is located at `repositoryUrl`. The content type of the project is specified by the `contentType` parameter. The chatbot is designed to provide conversational answers with hyperlinks back to GitHub, including code examples and links to the examples where appropriate. The target audience for the chatbot is specified by the `targetAudience` parameter.\n\nThe `makeChain` function takes several parameters:\n\n- `projectName`: The name of the software project.\n- `repositoryUrl`: The URL of the project's repository.\n- `contentType`: The type of content the chatbot is trained on.\n- `chatPrompt`: Additional instructions for answering questions about the content type.\n- `targetAudience`: The intended audience for the chatbot's answers.\n- `vectorstore`: An instance of HNSWLib for efficient nearest neighbor search.\n- `llms`: An array of LLMModels, which are language models used for generating answers.\n- `onTokenStream`: An optional callback function that is called when a new token is generated by the language model.\n\nThe `makeChain` function first creates a question generator using the `LLMChain` class. This generator is responsible for rephrasing follow-up questions to be standalone questions. It uses the `CONDENSE_PROMPT` template, which is defined at the beginning of the code.\n\nNext, the function creates a `QA_PROMPT` template using the `makeQAPrompt` function. This template is used to generate answers to the questions in a conversational manner, with hyperlinks back to GitHub and code examples where appropriate.\n\nFinally, the function creates and returns a new instance of the `ChatVectorDBQAChain` class, which combines the question generator and the document chain to create a chatbot that can answer questions about the software project. The chatbot uses the `vectorstore` for efficient nearest neighbor search and the `llms` language models for generating answers. If the `onTokenStream` callback is provided, it will be called when a new token is generated by the language model.",
          "questions": "1. **Question:** What is the purpose of the `makeChain` function and what are its input parameters?\n\n   **Answer:** The `makeChain` function is used to create a `ChatVectorDBQAChain` instance, which is responsible for generating questions and answers based on the given input parameters. The input parameters include `projectName`, `repositoryUrl`, `contentType`, `chatPrompt`, `targetAudience`, `vectorstore`, `llms`, and an optional `onTokenStream` function.\n\n2. **Question:** What are the roles of `CONDENSE_PROMPT` and `QA_PROMPT` in this code?\n\n   **Answer:** `CONDENSE_PROMPT` is a template for generating standalone questions from a given chat history and follow-up question. `QA_PROMPT` is a template for generating conversational answers with hyperlinks to GitHub, based on the provided context and question. Both templates are used in the `LLMChain` and `loadQAChain` instances, respectively.\n\n3. **Question:** How does the `onTokenStream` function work and when is it used?\n\n   **Answer:** The `onTokenStream` function is an optional callback that can be provided to the `makeChain` function. It is used to handle the streaming of tokens generated by the OpenAIChat instance. If provided, it will be called with each new token generated during the chat process.",
          "checksum": "6869048a06de62499933b14c37cddc1d"
        },
        {
          "fileName": "index.ts",
          "filePath": "src\\cli\\commands\\query\\index.ts",
          "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\query\\index.ts",
          "summary": "This code defines a chatbot interface for the Autodoc project, which allows users to ask questions related to a specific codebase and receive answers in a conversational manner. The chatbot uses a combination of the `inquirer` library for user input, `marked` and `marked-terminal` for rendering Markdown output, and the `langchain` library for handling natural language processing tasks.\n\nThe `query` function is the main entry point for the chatbot. It takes two arguments: an `AutodocRepoConfig` object containing information about the code repository, and an `AutodocUserConfig` object containing user-specific settings. The function initializes a vector store using the `HNSWLib` and `OpenAIEmbeddings` classes, and creates a chat chain using the `makeChain` function.\n\nThe chatbot interface is displayed using the `displayWelcomeMessage` function, which prints a welcome message to the console. The `getQuestion` function is used to prompt the user for a question using the `inquirer` library. The chatbot then enters a loop, where it processes the user's question, generates a response using the chat chain, and displays the response as Markdown in the terminal.\n\nIf an error occurs during the processing of a question, the chatbot will display an error message and continue to prompt the user for a new question. The loop continues until the user types 'exit', at which point the chatbot terminates.\n\nHere's an example of how the `query` function might be used:\n\n```javascript\nimport { query } from './autodoc';\n\nconst repoConfig = {\n  name: 'MyProject',\n  repositoryUrl: 'https://github.com/user/myproject',\n  output: 'path/to/output',\n  contentType: 'code',\n  chatPrompt: 'Ask me anything about MyProject',\n  targetAudience: 'developers',\n};\n\nconst userConfig = {\n  llms: 'path/to/llms',\n};\n\nquery(repoConfig, userConfig);\n```\n\nThis example would initialize the chatbot with the specified repository and user configurations, and start the chatbot interface for the user to ask questions about the \"MyProject\" codebase.",
          "questions": "1. **What is the purpose of the `query` function in this code?**\n\n   The `query` function is responsible for handling user interactions with the chatbot. It takes in an AutodocRepoConfig object and an AutodocUserConfig object, sets up the necessary data structures, and then enters a loop where it prompts the user for questions, processes them, and displays the results.\n\n2. **How does the code handle rendering Markdown text in the terminal?**\n\n   The code uses the `marked` library along with a custom `TerminalRenderer` to render Markdown text in the terminal. The `marked` library is configured with the custom renderer using `marked.setOptions({ renderer: new TerminalRenderer() });`.\n\n3. **What is the purpose of the `chatHistory` variable and how is it used?**\n\n   The `chatHistory` variable is an array that stores the history of questions and answers in the chat session. It is used to keep track of the conversation between the user and the chatbot. When a new question is asked, the chat history is passed to the `chain.call()` function, and the new question and its corresponding answer are added to the `chatHistory` array.",
          "checksum": "19807a33957666422f31136970c37245"
        }
      ],
      "folders": [],
      "summary": "The `query` folder in the Autodoc project contains code for creating a chatbot that can answer questions about a specific software project in a conversational manner. The chatbot is trained on the content of the project and provides answers with hyperlinks back to GitHub, including code examples and links to the examples where appropriate.\n\nThe main entry point for the chatbot is the `query` function in `index.ts`. It takes two arguments: an `AutodocRepoConfig` object containing information about the code repository, and an `AutodocUserConfig` object containing user-specific settings. The function initializes a vector store and creates a chat chain using the `makeChain` function from `createChatChain.ts`.\n\nHere's an example of how the `query` function might be used:\n\n```javascript\nimport { query } from './autodoc';\n\nconst repoConfig = {\n  name: 'MyProject',\n  repositoryUrl: 'https://github.com/user/myproject',\n  output: 'path/to/output',\n  contentType: 'code',\n  chatPrompt: 'Ask me anything about MyProject',\n  targetAudience: 'developers',\n};\n\nconst userConfig = {\n  llms: 'path/to/llms',\n};\n\nquery(repoConfig, userConfig);\n```\n\nThis example initializes the chatbot with the specified repository and user configurations and starts the chatbot interface for the user to ask questions about the \"MyProject\" codebase.\n\nThe `createChatChain.ts` file defines the `makeChain` function, which creates a chatbot for answering questions about a software project. The chatbot is designed to provide conversational answers with hyperlinks back to GitHub, including code examples and links to the examples where appropriate. The target audience for the chatbot is specified by the `targetAudience` parameter.\n\nThe `makeChain` function takes several parameters, such as `projectName`, `repositoryUrl`, `contentType`, `chatPrompt`, `targetAudience`, `vectorstore`, `llms`, and `onTokenStream`. It first creates a question generator using the `LLMChain` class, then creates a `QA_PROMPT` template using the `makeQAPrompt` function, and finally creates and returns a new instance of the `ChatVectorDBQAChain` class, which combines the question generator and the document chain to create a chatbot that can answer questions about the software project.\n\nIn summary, the code in the `query` folder is responsible for creating a chatbot that can answer questions about a specific software project in a conversational manner. The chatbot uses a combination of natural language processing techniques and efficient nearest neighbor search to generate accurate and relevant answers for the user.",
      "questions": "",
      "checksum": "9e0d0f111bf588e2df66862dce9db288"
    },
    {
      "folderName": "user",
      "folderPath": ".autodoc\\docs\\json\\src\\cli\\commands\\user",
      "url": "https://github.com/context-labs/autodoc/.autodoc\\docs\\json\\src\\cli\\commands\\user",
      "files": [
        {
          "fileName": "index.ts",
          "filePath": "src\\cli\\commands\\user\\index.ts",
          "url": "https://github.com/context-labs/autodoc/src\\cli\\commands\\user\\index.ts",
          "summary": "This code is responsible for managing the user configuration for the Autodoc project. It provides a way to create, update, and save the user configuration file, which stores information about the user's access to different Language Learning Models (LLMs) such as GPT-3, GPT-4, and GPT-4 32K.\n\nThe `makeConfigTemplate` function is used to create a default configuration object with the provided `config` parameter or with GPT-3 as the default LLM. This function is used to generate a new configuration object when needed.\n\nThe main function, `user`, is an asynchronous function that takes an optional `config` parameter. It first checks if a user configuration file already exists at the `userConfigFilePath`. If it does, the user is prompted to confirm whether they want to overwrite the existing configuration. If the user chooses not to overwrite, the process exits.\n\nIf the user configuration file does not exist, the code attempts to create the necessary directories for the file. If there's an error during this process, it logs the error and exits with a non-zero status code.\n\nNext, the user is prompted to select which LLMs they have access to. The available options are GPT-3.5 Turbo, GPT-3.5 Turbo with GPT-4 8K (Early Access), and GPT-3.5 Turbo with GPT-4 8K and GPT-4 32K (Early Access). The user's selection is then used to create a new configuration object using the `makeConfigTemplate` function.\n\nFinally, the new configuration object is written to the user configuration file in JSON format. A success message is displayed to the user, indicating that the configuration has been saved and they can start querying using the `doc q` command.\n\nExample usage:\n\n```javascript\nimport { user } from './path/to/this/file';\n\n// Create a new user configuration with default settings\nawait user();\n\n// Update the user configuration with a custom config object\nawait user({ llms: [LLMModels.GPT3, LLMModels.GPT4] });\n```",
          "questions": "1. **What is the purpose of the `makeConfigTemplate` function?**\n\n   The `makeConfigTemplate` function is used to create a default configuration object for the Autodoc user. It takes an optional `config` parameter and returns an object with a `llms` property, which is an array of LLM models.\n\n2. **How does the `user` function handle existing user configuration files?**\n\n   The `user` function checks if a user configuration file already exists using `fsSync.existsSync`. If it does, the user is prompted with a confirmation message to overwrite the existing configuration. If the user chooses not to overwrite, the process exits with a status code of 0.\n\n3. **What are the available choices for LLM models in the `user` function?**\n\n   The available choices for LLM models are GPT-3.5 Turbo, GPT-3.5 Turbo and GPT-4 8K (Early Access), and GPT-3.5 Turbo, GPT-4 8K (Early Access), and GPT-4 32K (Early Access). The user can select one of these options, and the selected value is stored in the `llms` property of the new configuration object.",
          "checksum": "76bc1e6d5d61e24907832c4cac443225"
        }
      ],
      "folders": [],
      "summary": "The `index.ts` file in the `user` folder is responsible for managing the user configuration for the Autodoc project. It allows users to create, update, and save their configuration file, which stores information about their access to different Language Learning Models (LLMs) such as GPT-3, GPT-4, and GPT-4 32K.\n\nThe `makeConfigTemplate` function creates a default configuration object with either the provided `config` parameter or GPT-3 as the default LLM. This function is useful for generating a new configuration object when needed.\n\nThe main function, `user`, is an asynchronous function that takes an optional `config` parameter. It first checks if a user configuration file already exists at the `userConfigFilePath`. If it does, the user is prompted to confirm whether they want to overwrite the existing configuration. If the user chooses not to overwrite, the process exits.\n\nIf the user configuration file does not exist, the code attempts to create the necessary directories for the file. If there's an error during this process, it logs the error and exits with a non-zero status code.\n\nNext, the user is prompted to select which LLMs they have access to. The available options are GPT-3.5 Turbo, GPT-3.5 Turbo with GPT-4 8K (Early Access), and GPT-3.5 Turbo with GPT-4 8K and GPT-4 32K (Early Access). The user's selection is then used to create a new configuration object using the `makeConfigTemplate` function.\n\nFinally, the new configuration object is written to the user configuration file in JSON format. A success message is displayed to the user, indicating that the configuration has been saved and they can start querying using the `doc q` command.\n\nThis code is essential for the Autodoc project as it allows users to manage their access to different LLMs and store their preferences in a configuration file. This configuration file can then be used by other parts of the project to determine which LLMs the user has access to and tailor the querying process accordingly.\n\nExample usage:\n\n```javascript\nimport { user } from './path/to/this/file';\n\n// Create a new user configuration with default settings\nawait user();\n\n// Update the user configuration with a custom config object\nawait user({ llms: [LLMModels.GPT3, LLMModels.GPT4] });\n```\n\nIn summary, the `index.ts` file in the `user` folder is a crucial part of the Autodoc project, allowing users to manage their LLM access and preferences. This configuration is then used by other parts of the project to provide a tailored experience based on the user's access to different LLMs.",
      "questions": "",
      "checksum": "4b8fd2b2abaec4959873fc3396c414d8"
    }
  ],
  "summary": "The code in the `.autodoc\\docs\\json\\src\\cli\\commands` folder is responsible for various tasks related to the Autodoc project, such as initializing the configuration, processing repositories, generating documentation, and creating a chatbot for answering questions about a specific software project. The folder contains several subfolders, each with a specific purpose.\n\n### estimate\n\nThe `estimate` function provides an estimated cost of processing a given repository. It takes an `AutodocRepoConfig` object as input and performs a dry run of the repository processing to calculate the estimated cost. Example usage:\n\n```javascript\nimport { estimate } from './path/to/this/file';\n\nconst config = {\n  name: 'my-repo',\n  repositoryUrl: 'https://github.com/user/my-repo.git',\n  root: './',\n  output: './output',\n  llms: ['en'],\n  ignore: ['.git', 'node_modules'],\n  filePrompt: true,\n  folderPrompt: true,\n  chatPrompt: true,\n  contentType: 'code',\n  targetAudience: 'developers',\n  linkHosted: true,\n};\n\nestimate(config);\n```\n\n### index\n\nThe code in this folder processes a given repository and generates documentation in JSON, Markdown, and vector formats. It takes an `AutodocRepoConfig` object as input and performs three main tasks: processing the repository, creating Markdown files, and creating vector files. Example usage:\n\n```javascript\nindex({\n  name: \"myProject\",\n  root: \"./input\",\n  output: \"./output\",\n  filePrompt: true,\n  folderPrompt: true,\n  contentType: \"code\",\n  targetAudience: \"developers\",\n  linkHosted: \"https://github.com/user/myProject\",\n});\n```\n\n### init\n\nThe `init` function initializes the configuration of the Autodoc project. It prompts the user to input necessary information to set up the project and creates the `autodoc.config.json` file in the project root. Example usage:\n\n```javascript\nimport { init } from './path/to/this/file';\n\n// Initialize the configuration with default values\nawait init();\n\n// Initialize the configuration with custom values\nawait init({\n  name: 'My Custom Repository',\n  repositoryUrl: 'https://github.com/user/repo',\n});\n```\n\n### query\n\nThe `query` folder contains code for creating a chatbot that can answer questions about a specific software project. The main entry point is the `query` function, which takes an `AutodocRepoConfig` object and an `AutodocUserConfig` object as input. Example usage:\n\n```javascript\nimport { query } from './autodoc';\n\nconst repoConfig = {\n  name: 'MyProject',\n  repositoryUrl: 'https://github.com/user/myproject',\n  output: 'path/to/output',\n  contentType: 'code',\n  chatPrompt: 'Ask me anything about MyProject',\n  targetAudience: 'developers',\n};\n\nconst userConfig = {\n  llms: 'path/to/llms',\n};\n\nquery(repoConfig, userConfig);\n```\n\n### user\n\nThe `user` folder manages the user configuration for the Autodoc project. It allows users to create, update, and save their configuration file, which stores information about their access to different Language Learning Models (LLMs). Example usage:\n\n```javascript\nimport { user } from './path/to/this/file';\n\n// Create a new user configuration with default settings\nawait user();\n\n// Update the user configuration with a custom config object\nawait user({ llms: [LLMModels.GPT3, LLMModels.GPT4] });\n```\n\nIn summary, the code in this folder is essential for various tasks related to the Autodoc project, such as initializing the configuration, processing repositories, generating documentation, and creating a chatbot for answering questions about a specific software project.",
  "questions": "",
  "checksum": "d11f941351fb51140313ada9b52bbf1a"
}